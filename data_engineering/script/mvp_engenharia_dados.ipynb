{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f8a9d5-49fe-4b84-9bf5-5569cbf25d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Biblioteca(s)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8acaa495-1b01-4d6e-89a1-4d55dd649199",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e8e273-e76b-45c0-bbc7-e247c6706a3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "666d6076-2885-4f9a-8301-2689318c062a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar bibliotecas...\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from geopy.distance import great_circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9424e366-6d2e-4498-8e9d-23ce943627c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Conectar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "055b9fa4-ffd2-490b-9560-079fbb952a71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Conectar-se ao Data Lake...\n",
    "dbutils.fs.mount( \n",
    "    source = \"wasbs://dados@engdadosmvp.blob.core.windows.net\", \n",
    "    mount_point = \"/mnt/dados\",\n",
    "    extra_configs = {\"fs.azure.account.key.engdadosmvp.blob.core.windows.net\":\"7uTSqaaFzINeCJ27Kmk+LOlzlJANBf2uupuOoy8MdgC8yg2Gggkaq7F6aXqvcodWcOIwKNowmcsE+AStKlIeSA==\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f1e3e2-d164-4c8a-bbae-6c4e7f3754e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Ler..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6453411c-058a-47f8-88cb-9baa1944060d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/dados/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c649d7-9f04-47a1-a600-23c1f2af0dae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####github..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c7001a-abcd-44d4-9409-b089196267e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_coordenadas_dos_municipios = spark.read.format(\"csv\")\\\n",
    "                                          .option(\"sep\", \",\")\\\n",
    "                                          .option(\"header\", \"true\")\\\n",
    "                                          .option(\"inferSchema\", \"true\")\\\n",
    "                                          .load(\"/mnt/dados/municipios.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ecaebdc-9c74-4efe-af8d-ef891584179e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####ibge..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1546113e-dbb8-4f6f-a9fe-403866688c86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pop_censo_2022 = pd.read_excel('/dbfs/mnt/dados/POP2022_Municipios_20230622.xls',\n",
    "                    skiprows=1,\n",
    "                            dtype={\n",
    "                                'COD. UF':str,\n",
    "                                'COD. MUNIC':str,\n",
    "                                'POPULAÇÃO':str\n",
    "                            }                   \n",
    ").dropna(thresh=2)\n",
    "\n",
    "df_area_munic = pd.read_excel('/dbfs/mnt/dados/AR_BR_RG_UF_RGINT_MES_MIC_MUN_2022.xls', \n",
    "                      sheet_name='AR_BR_MUN_2022', \n",
    "                      usecols=[4, 6]\n",
    ")\n",
    "\n",
    "df_inflacao_acumulada = pd.read_csv(\"/dbfs/mnt/dados/20230827084323.csv\",\n",
    "                        sep = ';',\n",
    "                        skiprows=2,\n",
    "                        names=['ano', 'inflacao_acumulada']\n",
    ").dropna()\n",
    "\n",
    "df_taxa_desemprego_media = pd.read_csv(\"/dbfs/mnt/dados/20230827084621.csv\",\n",
    "                        sep = ';',\n",
    "                        skiprows=1,\n",
    ").dropna().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e1e7306-3820-4503-862e-a4e5968eec19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####inmet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "422a2f33-5da0-4000-b036-ddcfcdd47c88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PASTA_CAMINHO = '/dbfs/mnt/dados/'\n",
    "ARQUIVOS = ['2019', '2020', '2021', '2022']\n",
    "\n",
    "df_estacoes_meteorologicas = pd.read_csv(f'{PASTA_CAMINHO}{\"estacoes_meteorologicas_br.csv\"}',\n",
    "                             sep=';'\n",
    ")\n",
    "\n",
    "def processar_arquivos(caminho_arquivo):\n",
    "    tabelas = []\n",
    "    nomes_arquivos = []\n",
    "    total_linhas = []\n",
    "\n",
    "    with zipfile.ZipFile(caminho_arquivo) as z:\n",
    "        for nome_arquivo_interno in z.namelist():\n",
    "            if nome_arquivo_interno.endswith('.CSV'):\n",
    "                with z.open(nome_arquivo_interno) as f:\n",
    "                    df = pd.read_csv(\n",
    "                        f,\n",
    "                        sep=';',\n",
    "                        skiprows=8,\n",
    "                        encoding='cp1252',\n",
    "                        usecols=[0, 2, 10],\n",
    "                        dtype={0: 'str', 2: 'float', 10: 'float'},\n",
    "                        decimal=','\n",
    "                    )\n",
    "\n",
    "                    df['Nome_Arquivo'] = nome_arquivo_interno\n",
    "                    nomes_arquivos.append(nome_arquivo_interno)\n",
    "                    tabelas.append(df)\n",
    "                    total_linhas.append(len(df))\n",
    "\n",
    "    return tabelas, nomes_arquivos, total_linhas \n",
    "\n",
    "def main():\n",
    "    tabelas_total = []\n",
    "    nomes_arquivos_total = []\n",
    "    df_log_data = []\n",
    "\n",
    "    for nome_arquivo in os.listdir(PASTA_CAMINHO):\n",
    "        if any(ano in nome_arquivo for ano in ARQUIVOS) and nome_arquivo.endswith('.zip'):\n",
    "            caminho_zip = os.path.join(PASTA_CAMINHO, nome_arquivo)\n",
    "            tabelas, nomes_arquivos, total_linhas = processar_arquivos(caminho_zip)\n",
    "\n",
    "            df_log_data.extend([{'nomes_arquivos_lidos': nome_arquivo, 'total_linhas': total_linhas_tabela}\n",
    "                                for nome_arquivo, total_linhas_tabela in zip(nomes_arquivos, total_linhas)])\n",
    "\n",
    "            tabelas_total.extend(tabelas)\n",
    "            nomes_arquivos_total.extend(nomes_arquivos)\n",
    "\n",
    "    df_log = pd.DataFrame(df_log_data)\n",
    "    return tabelas_total\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tabelas_total = main()\n",
    "    df_inmet = pd.concat(tabelas_total, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4641c13a-7a99-447f-8100-ae688857cad4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####ufmg..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51920502-41bf-4ba2-b776-6f01548ff005",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_moradores_rua = spark.read.format(\"csv\")\\\n",
    "                                  .option(\"sep\", \";\")\\\n",
    "                                  .option(\"header\", \"true\")\\\n",
    "                                  .option(\"inferSchema\", \"true\")\\\n",
    "                                  .load(\"/mnt/dados/municipios_serie_historica_pop_rua.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377d1a13-3802-400f-9f16-45b8298fdea6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####suas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfc9955-f336-4ca0-bd70-e2ef47d8b075",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pop_censo_2010 = spark.read.format(\"csv\")\\\n",
    "                                          .option(\"sep\", \";\")\\\n",
    "                                          .option(\"header\", \"true\")\\\n",
    "                                          .option(\"inferSchema\", \"true\") \\\n",
    "                                          .load(\"/mnt/dados/Lista_Municípios_com_IBGE_Brasil_Versao_CSV.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097badf3-dcaf-4218-bcb1-4d79717c8e6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###ETL..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85b118f1-e242-490d-8c49-cafbafb54700",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####github..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86498cf0-8113-4c83-b638-7d4fe0e4488c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_coord_municipios = df_coordenadas_dos_municipios.toPandas()\n",
    "\n",
    "manter_colunas = ['ESTACAO','LATITUDE', 'UF', 'LONGITUDE', \n",
    "                     'dias_temp_13_ano', 'dias_precip_50mm_ano']  \n",
    "\n",
    "df_unificado = pd.merge(df_estacoes_meteorologicas, resumo_temperatura_baixa, left_on='ESTACAO', right_on='Nome_Arquivo', how='left')\n",
    "df_unificado = pd.merge(df_unificado, resumo_pluviosidade, left_on='ESTACAO', right_on='Nome_Arquivo', how='left')\n",
    "\n",
    "df_unificado = df_unificado[manter_colunas]\n",
    "df_unificado['ESTACAO'] = df_unificado['ESTACAO'].apply(lambda x: \"ESTACAO METEOROLOGICA \" + x)\n",
    "df_unificado['LATITUDE'] = df_unificado['LATITUDE'].str.replace(',', '.')\n",
    "df_unificado['LONGITUDE'] = df_unificado['LONGITUDE'].str.replace(',', '.')\n",
    "\n",
    "def encontrar_estacao_mais_proxima(linha):\n",
    "    distancias = df_unificado.apply(\n",
    "        lambda r: great_circle((linha['latitude'], linha['longitude']), (r['LATITUDE'], r['LONGITUDE'])).kilometers,\n",
    "        axis=1\n",
    "    )\n",
    "    indice_mais_proximo = distancias.idxmin()\n",
    "    linha_mais_proxima = df_unificado.loc[indice_mais_proximo]\n",
    "    return (\n",
    "        linha_mais_proxima['ESTACAO'],\n",
    "        linha_mais_proxima['dias_temp_13_ano'],\n",
    "        linha_mais_proxima['dias_precip_50mm_ano']\n",
    "    )\n",
    "\n",
    "resultados = df_coord_municipios.apply(encontrar_estacao_mais_proxima, axis=1)\n",
    "df_coord_municipios[[\n",
    "    'ESTACAO', 'dias_temp_13_ano', 'dias_precip_50mm_ano'\n",
    "]] = pd.DataFrame(\n",
    "    resultados.to_list(),\n",
    "    columns=['ESTACAO', 'dias_temp_13_ano', 'dias_precip_50mm_ano']\n",
    ")\n",
    "\n",
    "df_coord_municipios = spark.createDataFrame(df_coord_municipios)\n",
    "df_coord_municipios = df_coord_municipios.withColumnRenamed('ESTACAO', 'estacoes_meteorologicas')\n",
    "df_coord_municipios.createOrReplaceTempView('DIM_COORD_MUNICIPIOS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a21c800-67b4-4a16-a4dc-5e1fb1402b50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####ibge..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f446aa50-85ba-46a1-b674-3037733a2b1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<command-3028181459309477>:1: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_pop_censo_2022['POPULAÇÃO'] = df_pop_censo_2022['POPULAÇÃO'].str.replace('.', '')\n"
     ]
    }
   ],
   "source": [
    "df_pop_censo_2022['POPULAÇÃO'] = df_pop_censo_2022['POPULAÇÃO'].str.replace('.', '')\n",
    "df_pop_censo_2022['POPULAÇÃO'] = df_pop_censo_2022['POPULAÇÃO'].str.split('(', expand=True)[0]\n",
    "df_pop_censo_2022['POPULAÇÃO'] = df_pop_censo_2022['POPULAÇÃO'].astype(int)\n",
    "\n",
    "df_pop_censo_2022[\"id_municipio\"] = df_pop_censo_2022[\"COD. UF\"] + df_pop_censo_2022[\"COD. MUNIC\"]\n",
    "df_pop_censo_2022.rename(columns={'POPULAÇÃO': 'populacao_censo_2022'}, inplace=True)\n",
    "df_pop_censo_2022['populacao_censo_2022'] = df_pop_censo_2022['populacao_censo_2022'].astype(float).astype(int)\n",
    "df_pop_censo_2022 = spark.createDataFrame(df_pop_censo_2022)\n",
    "df_pop_censo_2022.createOrReplaceTempView('DIM_POP_CENSO_2022')\n",
    "\n",
    "df_area_munic = spark.createDataFrame(df_area_munic)\n",
    "df_area_munic.createOrReplaceTempView('DIM_AREA_MUNIC')\n",
    "\n",
    "df_inflacao_acumulada['ano'] = df_inflacao_acumulada['ano'].str.split().str[-1]\n",
    "df_inflacao_acumulada = spark.createDataFrame(df_inflacao_acumulada)\n",
    "df_inflacao_acumulada.createOrReplaceTempView('DIM_INFLACAO_ACUMULADA')\n",
    "\n",
    "df_taxa_desemprego_media = df_taxa_desemprego_media.reset_index()\n",
    "df_taxa_desemprego_media.columns = ['ano', 'taxa_desemprego']\n",
    "df_taxa_desemprego_media['ano'] = df_taxa_desemprego_media['ano'].str.split().str[-1]\n",
    "df_taxa_desemprego_media = df_taxa_desemprego_media.groupby(['ano'])['taxa_desemprego'].mean().reset_index()\n",
    "df_taxa_desemprego_media.rename(columns={'taxa_desemprego': 'taxa_desemprego_media'}, inplace=True)\n",
    "df_taxa_desemprego_media = spark.createDataFrame(df_taxa_desemprego_media)\n",
    "df_taxa_desemprego_media.createOrReplaceTempView('DIM_TAXA_DESEMPREGO_MEDIA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39b0a477-316d-4d1a-be8f-591734c5c524",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####inmet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516c27c2-db10-49ab-a7e7-8b3844b540c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transformar_nome_arquivo(nome):\n",
    "    return nome.split('_')[4]\n",
    "\n",
    "def main():\n",
    "    df_inmet['Data'] = pd.to_datetime(df_inmet['Data'])\n",
    "    df_inmet['Ano'] = df_inmet['Data'].dt.year\n",
    "\n",
    "    pluviosidade_por_dia_filtrado = (\n",
    "        df_inmet.groupby(['Data', 'Ano', 'Nome_Arquivo'])['PRECIPITAÇÃO TOTAL, HORÁRIO (mm)']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    pluviosidade_por_dia_filtrado = pluviosidade_por_dia_filtrado[pluviosidade_por_dia_filtrado['PRECIPITAÇÃO TOTAL, HORÁRIO (mm)'] > 50]\n",
    "    pluviosidade_por_dia_filtrado['Nome_Arquivo'] = pluviosidade_por_dia_filtrado['Nome_Arquivo'].apply(transformar_nome_arquivo)\n",
    "    pluviosidade_por_dia_filtrado['id_dt_municipio'] = pluviosidade_por_dia_filtrado['Data'].astype(str) + pluviosidade_por_dia_filtrado['Nome_Arquivo']\n",
    "    pluviosidade_por_dia_filtrado.drop_duplicates(subset='id_dt_municipio', inplace=True)\n",
    "    \n",
    "    resumo_pluviosidade = (\n",
    "        pluviosidade_por_dia_filtrado.groupby(['Nome_Arquivo'])\n",
    "        .size()\n",
    "        .reset_index(name='dias_precip_>50mm')\n",
    "    )\n",
    "    \n",
    "    resumo_pluviosidade['dias_precip_50mm_ano'] = (resumo_pluviosidade['dias_precip_>50mm']/len(ARQUIVOS)).astype(str)\n",
    "    resumo_pluviosidade['dias_precip_50mm_ano'] = resumo_pluviosidade['dias_precip_50mm_ano'].str.replace('.', ',')\n",
    "\n",
    "    temperatura_abaixo_de_13 = df_inmet[df_inmet['TEMPERATURA MÍNIMA NA HORA ANT. (AUT) (°C)'] < 13]\n",
    "    temperatura_abaixo_de_13['Nome_Arquivo'] = temperatura_abaixo_de_13['Nome_Arquivo'].apply(transformar_nome_arquivo)\n",
    "    temperatura_abaixo_de_13['Data'] = temperatura_abaixo_de_13['Data'].astype(str)\n",
    "    temperatura_abaixo_de_13['id_dt'] = temperatura_abaixo_de_13['Data'] + temperatura_abaixo_de_13['Nome_Arquivo']\n",
    "    temperatura_abaixo_de_13 = temperatura_abaixo_de_13.drop_duplicates(subset=['id_dt'])\n",
    "    \n",
    "    resumo_temperatura_baixa = (\n",
    "          temperatura_abaixo_de_13.groupby(['Nome_Arquivo'])\n",
    "          .size()\n",
    "          .reset_index(name='dias_temp_<13')\n",
    "    )\n",
    "\n",
    "    resumo_temperatura_baixa['dias_temp_13_ano'] = (resumo_temperatura_baixa['dias_temp_<13'] / len(ARQUIVOS))\n",
    "    limite_sup = resumo_temperatura_baixa['dias_temp_13_ano'].mean() + 3.5*resumo_temperatura_baixa['dias_temp_13_ano'].std()\n",
    "    limite_inf = resumo_temperatura_baixa['dias_temp_13_ano'].mean() - 3.5*resumo_temperatura_baixa['dias_temp_13_ano'].std()\n",
    "    resumo_temperatura_baixa = resumo_temperatura_baixa.loc[(resumo_temperatura_baixa['dias_temp_13_ano'] <= limite_sup) & (resumo_temperatura_baixa['dias_temp_13_ano'] >= limite_inf)]\n",
    "    resumo_temperatura_baixa['dias_temp_13_ano'] = resumo_temperatura_baixa['dias_temp_13_ano'].astype(str)\n",
    "    resumo_temperatura_baixa['dias_temp_13_ano'] = resumo_temperatura_baixa['dias_temp_13_ano'].str.replace('.', ',')\n",
    "\n",
    "    return resumo_temperatura_baixa, resumo_pluviosidade\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    resumo_temperatura_baixa, resumo_pluviosidade = main()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59791f66-acc4-4121-81f1-555fec083155",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####ufmg..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901d7fa1-3e66-4744-a0cf-01ba7b2f0846",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colunas_para_excluir = ['_c0']\n",
    "df_moradores_rua = df_moradores_rua.drop(*colunas_para_excluir)\n",
    "\n",
    "df_moradores_rua = df_moradores_rua.withColumnRenamed('codigo_ibg', 'id_municipio')\\\n",
    "                                             .withColumnRenamed('freq', 'moradores_rua')\\\n",
    "                                             .withColumnRenamed('ano', 'ano_levantamento')\n",
    "\n",
    "df_moradores_rua.createOrReplaceTempView(\"DIM_MORADORES_RUA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6365872e-d2c6-4c12-b0a1-b0c83f5256bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####suas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a289933-619a-4440-a5fd-e7ddb5de97a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colunas_para_excluir = ['ConcatUF+Mun', 'IBGE', 'Capital', '_c9']\n",
    "df_pop_censo_2010 = df_pop_censo_2010.drop(*colunas_para_excluir)\n",
    "\n",
    "df_pop_censo_2010 = df_pop_censo_2010.withColumnRenamed('IBGE7', 'id_municipio')\\\n",
    "                             .withColumnRenamed('UF', 'uf')\\\n",
    "                             .withColumnRenamed('Município', 'municipio')\\\n",
    "                             .withColumnRenamed('Região', 'regiao')\\\n",
    "                             .withColumnRenamed('População 2010', 'populacao_censo_2010')\\\n",
    "                             .withColumnRenamed('Porte', 'porte')\n",
    "                             \n",
    "df_pop_censo_2010.createOrReplaceTempView(\"MUNICIPIOS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c0e527-3f88-4559-9e11-d6e5b03b94b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Modelo estrela..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82ecaf1-0aeb-46c8-8cd2-f25b1a4d903c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação das tabelas dimensão...\n",
    "df_pop_censo_2010_id = spark.sql(\"\"\" SELECT municipio\n",
    "                          FROM MUNICIPIOS\n",
    "                          ORDER BY municipio\n",
    "\"\"\")\n",
    "\n",
    "df_pop_censo_2010_id = df_pop_censo_2010_id.withColumn('id_municipio', monotonically_increasing_id()+1)\n",
    "df_pop_censo_2010_id.createOrReplaceTempView('DIM_MUNICIPIOS')\n",
    "\n",
    "df_regiao = spark.sql(\"\"\" SELECT DISTINCT regiao\n",
    "                          FROM MUNICIPIOS\n",
    "                          ORDER BY regiao\n",
    "\"\"\")\n",
    "\n",
    "df_regiao = df_regiao.withColumn('id_regiao', monotonically_increasing_id()+1)\n",
    "df_regiao.createOrReplaceTempView('DIM_REGIAO')\n",
    "\n",
    "df_estado = spark.sql(\"\"\" SELECT DISTINCT uf\n",
    "                          FROM MUNICIPIOS\n",
    "                          ORDER BY uf\n",
    "\"\"\")\n",
    "\n",
    "df_estado = df_estado.withColumn('id_uf', monotonically_increasing_id()+1)\n",
    "df_estado.createOrReplaceTempView('DIM_UNIDADE_FEDERATIVA')\n",
    "\n",
    "df_porte = spark.sql(\"\"\" SELECT DISTINCT porte\n",
    "                          FROM MUNICIPIOS\n",
    "                          ORDER BY porte\n",
    "\"\"\")\n",
    "\n",
    "df_porte = df_porte.withColumn('id_porte', monotonically_increasing_id()+1)\n",
    "df_porte.createOrReplaceTempView('DIM_PORTE_MUNICIPIO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6354330-c3a4-4e81-b87f-5886ef37a936",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação da tabela fato...\n",
    "df_fato = spark.sql(\"\"\"\n",
    "                    SELECT \n",
    "                        M.id_municipio, U.id_uf, R.id_regiao, P.id_porte, C.latitude, C.longitude, AR.AR_MUN_2022,\n",
    "                        C.dias_temp_13_ano, C.dias_precip_50mm_ano, C.estacoes_meteorologicas,\n",
    "                        M.populacao_censo_2010, CXXII.populacao_censo_2022, MR.moradores_rua, INF.inflacao_acumulada, TD.taxa_desemprego_media, MR.ano_levantamento\n",
    "\n",
    "                        FROM MUNICIPIOS M\n",
    "                        LEFT JOIN DIM_MORADORES_RUA MR ON MR.id_municipio = M.id_municipio\n",
    "                        LEFT JOIN DIM_COORD_MUNICIPIOS C ON C.codigo_ibge = M.id_municipio\n",
    "                        LEFT JOIN DIM_POP_CENSO_2022 CXXII ON CXXII.id_municipio = M.id_municipio\n",
    "                        LEFT JOIN DIM_AREA_MUNIC AR ON AR.CD_MUN = M.id_municipio\n",
    "                        LEFT JOIN DIM_REGIAO R ON R.regiao = M.regiao\n",
    "                        LEFT JOIN DIM_UNIDADE_FEDERATIVA U ON U.uf = M.uf\n",
    "                        LEFT JOIN DIM_PORTE_MUNICIPIO P ON P.porte = M.porte\n",
    "                        LEFT JOIN DIM_INFLACAO_ACUMULADA INF ON INF.ano = MR.ano_levantamento\n",
    "                        LEFT JOIN DIM_TAXA_DESEMPREGO_MEDIA TD ON TD.ano = MR.ano_levantamento\n",
    "\n",
    "                        ORDER BY M.id_municipio\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79f97f4-47b5-4cc5-aaef-36f376e29252",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Carga..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c2709cc-7d8e-4fbc-85eb-bdb921d28c1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def carga_sql(df, tabela):\n",
    "    df.write.format(\"jdbc\")\\\n",
    "            .mode(\"append\")\\\n",
    "            .option(\"url\", \"jdbc:sqlserver://mouraxy.database.windows.net\")\\\n",
    "            .option(\"port\", \"1433\")\\\n",
    "            .option(\"user\", \"mouraxy\")\\\n",
    "            .option(\"password\", \"**********\")\\\n",
    "            .option(\"database\", \"bd_mouraxy\")\\\n",
    "            .option(\"dbtable\", tabela)\\\n",
    "            .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94704da4-b591-4f5f-bc72-2b82b6a636c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enviar dados para o SQL...\n",
    "#carga_sql(df_pop_censo_2010_id, \"DW.DIM_MUNICIPIOS\")\n",
    "#carga_sql(df_regiao, \"DW.DIM_REGIAO\")\n",
    "#carga_sql(df_estado, \"DW.DIM_UNIDADE_FEDERATIVA\")\n",
    "#carga_sql(df_porte, \"DW.DIM_PORTE_MUNICIPIO\")\n",
    "#carga_sql(df_fato, \"DW.MORADORES_SITUACAO_RUA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61e5d927-acfc-4b56-b772-0fbdfa09f166",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origem: part-00000-tid-2616322005217052032-98781045-9946-4705-83c3-0d2edbe09a85-79-1-c000.csv\n",
      "Destino: /mnt/dados/dw/FACT_POP_RUA.csv\n"
     ]
    }
   ],
   "source": [
    "# Enviar dados para o Data Lake...\n",
    "df_fato.coalesce(1)\\\n",
    "    .write\\\n",
    "    .format('csv')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .save('/mnt/dados/spark/')\n",
    "\n",
    "def mover_arquivos(extensao, origem, destino, novo_nome):\n",
    "    files = dbutils.fs.ls(origem)\n",
    "    for i in range (0, len(files)):\n",
    "        file = files[i].name\n",
    "        if extensao in file:\n",
    "            dbutils.fs.cp(files[i].path, destino+novo_nome+extensao)\n",
    "            print('Origem: '+file+'\\nDestino: '+destino+novo_nome+extensao)\n",
    "\n",
    "mover_arquivos('.csv', '/mnt/dados/spark/', '/mnt/dados/dw/', 'FACT_POP_RUA')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mvp_engenharia_dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
